{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# List documents text and add them to vector database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split documents (for larger datasets)\n",
    "documents = [\n",
    "    \"RAG stands for Retrieval-Augmented Generation.\",\n",
    "    \"The capital of France is Paris.\",\n",
    "    \"Machine learning enables AI models to improve over time.\",\n",
    "    \"Python is commonly used for data science and AI applications.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=20)\n",
    "docs = [{\"content\": doc} for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")  # Saves database locally\n",
    "collection = client.get_or_create_collection(name=\"rag_demo\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/kkc5ktp526qb9sgjlvzpdwh80000gn/T/ipykernel_39891/417115223.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/Users/shubhamdohare/Code/RAG_learning/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Add of existing embedding ID: 0\n",
      "Add of existing embedding ID: 1\n",
      "Add of existing embedding ID: 2\n",
      "Add of existing embedding ID: 3\n",
      "Insert of existing embedding ID: 0\n",
      "Add of existing embedding ID: 0\n",
      "Insert of existing embedding ID: 1\n",
      "Add of existing embedding ID: 1\n",
      "Insert of existing embedding ID: 2\n",
      "Add of existing embedding ID: 2\n",
      "Insert of existing embedding ID: 3\n",
      "Add of existing embedding ID: 3\n"
     ]
    }
   ],
   "source": [
    "# Use Sentence Transformers for embeddings\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Store documents as embeddings in ChromaDB\n",
    "for i, doc in enumerate(docs):\n",
    "    collection.add(ids=[str(i)], documents=[doc[\"content\"]], embeddings=[embedding_model.embed_query(doc[\"content\"])])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents: ['RAG stands for Retrieval-Augmented Generation.', 'Python is commonly used for data science and AI applications.']\n"
     ]
    }
   ],
   "source": [
    "def retrieve_top_k(query, k=2):\n",
    "    query_embedding = embedding_model.embed_query(query)\n",
    "    results = collection.query(query_embeddings=[query_embedding], n_results=k)\n",
    "    if results and results[\"documents\"]:\n",
    "        return results[\"documents\"][0]\n",
    "    return []\n",
    "\n",
    "# Test retrieval\n",
    "query = \"What is RAG?\"\n",
    "retrieved_context = retrieve_top_k(query)\n",
    "print(\"Retrieved Documents:\", retrieved_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use GPT for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Using following context:\n",
      "RAG stands for Retrieval-Augmented Generation.\n",
      "Answer the question: What is RAG?\n",
      "A repeating rusherer comes from the precomputed control that many of us have derived from our time spent in the sciences. A DRMN uses this control many times over its lifespan to partition an ALLC into additional subunits for analysis. As a result, as RAG are averaged over all our \"one-to-one\" lives, RAG will break down on a molecular level into every single LTSL LTSL RAG with a bounding mathematical term applicable within the LTSL.\n",
      "Take RAG sequenced from its earliest formation (destruction) and combined with 100 genes. Then four more lecithin and Cohamot-HeissÃ¨res proteins (amplify-reinfection-long-lived spindle protein) are inscribed. Colony weights of the six malaria genes show up over this extended length. About 80 alleles up to 11 are Merkel cells; 8 for Akkermans; 1 for Afrumans and 5 for Dakkarzone. The web of alignments that allows a plasmid to be identically mapped across generations, helps bring out our infordecity in the intervening few years (see RAG history).\n",
      "On average, 100 alleles of us will ever make up a search for \"rorting\" the 2010 gene database, literally trying to figure out how each â–º SELECT is based on which alleles it satisfies. Rendering it that way is not one of my regular issues; I say it often or many times. It is generally easier to decide 3 if we disconnect every right move from the Hiller effect without extra fumble if flipping work we done in infancy so early in life.\n",
      "What if we flip it off but not recognize when we first start pushing inputs of DGT and TCT or the NHG for which it was the last truly successful pickup? It's a beautiful dual Astrovillian. Everyone feels only escalating injectable limbs, but we pick various pedigrees, meeting each other but one separate grasp. We raise flowers on par with the crudest loveable local marshmallow inserts. Expect all of us to start brainstorming the same thing, cementing it together and sometimes even having to tidy anyway. We'd appreciate an update on soundbreezeOM's recent talk about the exact mechanisms that ensure surprises. They actually think we're using bottlenecking too much. Region hypohydrodynamics can be hell of a highway. Thankfully barefoot snowflakes, worked clouds, big umbrellas, scuffy hair, thin limbs free of hair strip bacon Fiambo with spiders and Centaurs, warm espresso delivered between mosquitoes and Smith to enable big worms. The worst weather (I've actually was in a gamble; wore acrylic to \"make\" the body). Short cold storms (numbers represent weekdays and international average are to be found from 1 to 25 weekends). We win because we enjoy living like you.\n",
      "(The 25 numbers are so volatile that the IPBU has decided emphatically not to release them.)\n",
      "What if a total of two sets of four different states disinhibit Blink Sound Transit? Does this mean certain airports will stay closed while a single Named Bab is?\"Put short side by side in (Hyde Oval. Cape Town. L.A.TRAIGHT.â€”*Iasertick.] Shove that Gibson Grover. Put long side by side in Reggies. Put VIXX Circus Fantamipids at Carnation. Try landscaping accurateigators. Develop a new ornamethrower beautifully calculated in the CA lab at your local Animal Foundation Hospitalâ€¦ Trying to value lovable digits an impossible task. Sourcing materials from one external test bed is essential to collect enough measurements to prove specific to a particular lab. Put another needle all over your lab.\n",
      "The Auburn Pebble sits in your prong with no mirrors. Â That site is now 5 Cs deep. You think that if Earl had a glitter to match his 270, he would match it. Â Since that piece strikes you most shades of orange, silver, yellow, burgundy, an abreatments to retail, Pound for pound good for a $75,000 dose until the mechwheels produce enough light to confirm the Romans knew it was gold (thanks to Cheshire Crush!) Herbert is such a serious option very early in life that, while I was never vivacious and accountants once disconnected a tap set so fast I nearly died; unfortunately, he died of cancer a long time ago, may need one day atlest, if the conditions get RIGHT. ðŸ˜„\n",
      "And, I'm not going to name a single entity with the right people. Even though the most politically correct eta/edna fanatics are lobbyists trying to join the cooling of Washington DC's chambers of commerce forcing regulation on foundations that have which are utterly off limits at the Constitutional level ? All join ESPY\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "API_KEY: str = os.getenv(\"HUGGING_FACE_KEY\", default=\"\")\n",
    "\n",
    "query = \"What is RAG?\"\n",
    "\n",
    "generated_prompt_with_context = retrieve_top_k(query, 1)\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/gpt2\"\n",
    "headers = {\"Authorization\": f\"Bearer {API_KEY}\"}\n",
    "\n",
    "rag_prompt = f\"\"\"\n",
    "Using following context:\n",
    "{\"\\n\".join(generated_prompt_with_context)}\\nAnswer the question: {query} \n",
    "\"\"\"\n",
    "\n",
    "payload = {\"inputs\": rag_prompt}\n",
    "response = requests.post(API_URL, headers=headers, json=payload)\n",
    "\n",
    "response_data = response.json()\n",
    "for choice in response_data[0][\"generated_text\"].split(\"\\n\"):\n",
    "    print(choice.replace(\"Answer:\", \"\").strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using GEMINI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google import genai\n",
    "\n",
    "# Initialize Bedrock client\n",
    "API_KEY: str = os.getenv(\"GEMINI_API_KEY\", default=\"\")\n",
    "\n",
    "rag_prompt = f\"\"\"\n",
    "Using following context:\n",
    "{\"\\n\".join(generated_prompt_with_context)}\\nAnswer the question: {query} \n",
    "\"\"\"\n",
    "\n",
    "client = genai.Client(api_key=API_KEY)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model='gemini-2.0-flash', contents=rag_prompt\n",
    ")\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
